{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784b32f1-093c-4345-8d28-b9586332c084",
   "metadata": {},
   "source": [
    "# Multi-level flow with fully-connected networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f15a43-2515-4efa-acce-e200d1319135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "#from jsonargparse.typing import PositiveInt, PositiveFloat, NonNegativeFloat\n",
    "\n",
    "import flows.phi_four as phi_four\n",
    "import flows.transforms as transforms\n",
    "import flows.utils as utils\n",
    "from flows.distributions import Prior, FreeScalarDistribution\n",
    "\n",
    "Tensor: TypeAlias = torch.Tensor\n",
    "BoolTensor: TypeAlias = torch.BoolTensor\n",
    "Module: TypeAlias = torch.nn.Module\n",
    "IterableDataset: TypeAlias = torch.utils.data.IterableDataset\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596907ea-226b-44ca-9ed7-c78f33c2edb8",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d3d55-6229-49f7-9a36-a8602e5da842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(Module):\n",
    "    def __init__(self, transform, net_spec: dict):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.net_a = self.build_convnet(**net_spec)\n",
    "        self.net_b = self.build_convnet(**net_spec)\n",
    "\n",
    "    def build_convnet(\n",
    "        self,\n",
    "        hidden_shape: tuple[PositiveInt],\n",
    "        activation: Module = torch.nn.Tanh(),\n",
    "        final_activation: Module = torch.nn.Identity(),\n",
    "        kernel_size: PositiveInt = 3,\n",
    "        use_bias: bool = True,\n",
    "    ):\n",
    "        net_shape = [1, *hidden_shape, self.transform.params_dof]\n",
    "        activations = [activation for _ in hidden_shape] + [final_activation]\n",
    "\n",
    "        net = []\n",
    "        for in_channels, out_channels, activation in zip(\n",
    "            net_shape[:-1], net_shape[1:], activations\n",
    "        ):\n",
    "            convolution = torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=1,\n",
    "                padding_mode=\"circular\",\n",
    "                stride=1,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "            net.append(convolution)\n",
    "            net.append(activation)\n",
    "\n",
    "        return torch.nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x_full: Tensor, log_det_jacob: tensor) -> tuple[Tensor]:\n",
    "        n_batch, n_channels, l1, l2 = x_full.shape\n",
    "        if n_channels > 1:\n",
    "            x, h = torch.tensor_split(x_full, [1], dim=1)  # take first channel\n",
    "        else:\n",
    "            x = x_full\n",
    "        mask = utils.make_checkerboard((l1, l2)).to(x.device)\n",
    "        mask_expanded = mask.view(1, 1, l1, l2)\n",
    "\n",
    "        x_a = x[..., mask]\n",
    "        x_b = x[..., ~mask]\n",
    "\n",
    "        params_a = self.net_b(x.mul(~mask_expanded))[..., mask]\n",
    "        y_a, log_det_jacob_a = self.transform(x_a, params_a)\n",
    "\n",
    "        xy = torch.zeros_like(x)\n",
    "        xy[..., mask] = y_a\n",
    "\n",
    "        params_b = self.net_a(xy)[..., ~mask]\n",
    "        y_b, log_det_jacob_b = self.transform(x_b, params_b)\n",
    "\n",
    "        y = xy.clone()\n",
    "        y[..., ~mask] = y_b\n",
    "\n",
    "        if n_channels > 1:\n",
    "            y_full = torch.cat([y, h], dim=1)\n",
    "        else:\n",
    "            y_full = y\n",
    "\n",
    "        log_det_jacob.add_(log_det_jacob_a)\n",
    "        log_det_jacob.add_(log_det_jacob_b)\n",
    "\n",
    "        return y_full, log_det_jacob\n",
    "\n",
    "\n",
    "class UpsamplingLayer(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kernel = torch.stack(\n",
    "            [\n",
    "                Tensor([[1, 0], [0, 0]]),\n",
    "                Tensor([[0, 1], [0, 0]]),\n",
    "                Tensor([[0, 0], [1, 0]]),\n",
    "                Tensor([[0, 0], [0, 1]]),\n",
    "            ],\n",
    "            dim=0,\n",
    "        ).unsqueeze(dim=1)\n",
    "        assert kernel.shape == torch.Size([4, 1, 2, 2])\n",
    "\n",
    "        self.register_buffer(\"kernel\", kernel)\n",
    "\n",
    "    def forward(self, x: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        \"\"\"Upsample 1 lattice site -> 4 lattice sites.\"\"\"\n",
    "        n_batch, _, l1, l2 = x.shape\n",
    "        assert (l1 % 2 == 0) and (l2 % 2 == 0)\n",
    "        y = F.conv_transpose2d(x.view(-1, 4, l1, l2), self.kernel, stride=2).view(\n",
    "            n_batch, -1, 2 * l1, 2 * l2\n",
    "        )\n",
    "        return y, log_det_jacob\n",
    "\n",
    "    def inverse(self, y: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        \"\"\"Downsample 4 lattice sites -> 1 lattice site.\"\"\"\n",
    "        n_batch, _, l1, l2 = y.shape\n",
    "        assert (l1 % 2 == 0) and (l2 % 2 == 0)\n",
    "        x = F.conv2d(y.view(-1, 1, l1, l2), self.kernel, stride=2).view(\n",
    "            n_batch, -1, l1 // 2, l2 // 2\n",
    "        )\n",
    "        return x, log_det_jacob\n",
    "\n",
    "\n",
    "_test_input = torch.arange(64).view(1, 1, 8, 8).float()\n",
    "_test_layer = UpsamplingLayer()\n",
    "_test_out1, _ = _test_layer.inverse(_test_input, None)\n",
    "_test_out2, _ = _test_layer.inverse(_test_out1, None)\n",
    "assert torch.allclose(_test_out2[0, 0], torch.Tensor([[0, 4], [32, 36]]))\n",
    "_test_out1_rt, _ = _test_layer.forward(_test_out2, None)\n",
    "assert torch.allclose(_test_out1, _test_out1_rt)\n",
    "_test_input_rt, _ = _test_layer.forward(_test_out1_rt, None)\n",
    "assert torch.allclose(_test_input, _test_input_rt)\n",
    "\n",
    "\n",
    "class GlobalRescalingLayer(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_scale = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, x: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        x.mul_(self.log_scale.exp())\n",
    "        numel = utils.prod(x.shape[1:])\n",
    "        log_det_jacob.add_(self.log_scale.mul(numel))\n",
    "        return x, log_det_jacob\n",
    "\n",
    "    def inverse(self, y: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        y.mul_(self.log_scale.neg().exp())\n",
    "        numel = utils.prod(y.shape[1:])\n",
    "        log_det_jacob.sub_(self.log_scale.mul(numel))\n",
    "        return y, log_det_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed813a-4a8b-4c30-a73f-e230056c0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilevelFlow(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        m_sq: float,\n",
    "        lam: NonNegativeFloat,\n",
    "        model_spec: list[dict | str],\n",
    "        # layers: list[Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        n_upsampling = 0\n",
    "        for layer_spec in reversed(model_spec):\n",
    "            if layer_spec == \"upsampling\":\n",
    "                layers.insert(0, UpsamplingLayer())\n",
    "                n_upsampling += 1\n",
    "            elif layer_spec == \"rescaling\":\n",
    "                layers.insert(0, GlobalRescalingLayer())\n",
    "            else:\n",
    "                transform = layer_spec[\"transform\"](**layer_spec[\"transform_spec\"])\n",
    "                layer = CouplingLayer(transform, layer_spec[\"net_spec\"])\n",
    "                layers.insert(0, layer)\n",
    "        layers.append(GlobalRescalingLayer())\n",
    "\n",
    "        self.flow = utils.Flow(*layers)\n",
    "        self.n_upsampling = n_upsampling\n",
    "        self.action = phi_four.PhiFourAction(m_sq, lam)\n",
    "\n",
    "        self.curr_iter = 0\n",
    "\n",
    "        self.upsampling_layer = UpsamplingLayer()\n",
    "\n",
    "    def _reshape_z(self, z):\n",
    "        for level in range(self.n_upsampling):\n",
    "            z, _ = self.upsampling_layer.inverse(z, None)\n",
    "        return z\n",
    "\n",
    "    def log_state(self, phi):\n",
    "        self.logger.experiment.add_histogram(\"phi\", phi.flatten(), self.curr_iter)\n",
    "        self.logger.experiment.add_histogram(\n",
    "            \"action\", self.action(phi).flatten(), self.curr_iter\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        z, log_prob_z = batch\n",
    "        z = self._reshape_z(z)\n",
    "        phi, log_det_jacob = self.flow(z)\n",
    "        weights = log_prob_z - log_det_jacob + self.action(phi)\n",
    "\n",
    "        self.curr_iter += 1\n",
    "        if self.curr_iter % 1000 == 0:\n",
    "            self.log_state(phi)\n",
    "\n",
    "        return phi, weights\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        self.lr_schedulers().step()\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        phi, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        acceptance = utils.metropolis_acceptance(weights)\n",
    "        metrics = dict(loss=loss, acceptance=acceptance)\n",
    "        self.log_dict(metrics, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.flow.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_steps\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prior: IterableDataset, n_iter: PositiveInt = 1):\n",
    "        phi, weights = self.forward(next(prior))\n",
    "        for _ in range(n_iter - 1):\n",
    "            _phi, _weights = self.forward(next(prior))\n",
    "            phi = torch.cat((phi, _phi), dim=0)\n",
    "            weights = torch.cat((weights, _weights), dim=0)\n",
    "        return phi, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197aac2a-d310-4a97-9818-6a130be5018a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a412e2-aaf1-444e-a47e-dedd2fad10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target theory\n",
    "LATTICE_LENGTH = 8\n",
    "M_SQ = 1\n",
    "LAM = 0\n",
    "\n",
    "# Model spec\n",
    "ADDITIVE_BLOCK = {\n",
    "    \"transform\": transforms.PointwiseAdditiveTransform,\n",
    "    \"transform_spec\": {},\n",
    "    \"net_spec\": {\n",
    "        \"hidden_shape\": [4, 4],\n",
    "        \"activation\": torch.nn.Tanh(),\n",
    "        \"final_activation\": torch.nn.Identity(),\n",
    "        \"use_bias\": False,\n",
    "    },\n",
    "}\n",
    "AFFINE_BLOCK = {\n",
    "    \"transform\": transforms.PointwiseAffineTransform,\n",
    "    \"transform_spec\": {},\n",
    "    \"net_spec\": {\n",
    "        \"hidden_shape\": [4, 4],\n",
    "        \"activation\": torch.nn.Tanh(),\n",
    "        \"final_activation\": torch.nn.Tanh(),\n",
    "        \"use_bias\": False,\n",
    "    },\n",
    "}\n",
    "SPLINE_BLOCK = {\n",
    "    \"transform\": transforms.PointwiseRationalQuadraticSplineTransform,\n",
    "    \"transform_spec\": {\"n_segments\": 8},\n",
    "    \"net_spec\": {\n",
    "        \"hidden_shape\": [4],\n",
    "        \"activation\": torch.nn.Tanh(),\n",
    "        \"final_activation\": torch.nn.Identity(),\n",
    "        \"use_bias\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_SPEC = [\n",
    "    AFFINE_BLOCK,\n",
    "    # \"upsampling\",\n",
    "    AFFINE_BLOCK,\n",
    "    # \"upsampling\",\n",
    "    AFFINE_BLOCK,\n",
    "    \"rescaling\",\n",
    "]\n",
    "\n",
    "N_TRAIN = 5000\n",
    "N_BATCH = 1000\n",
    "N_BATCH_VAL = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3334e-c84f-4c17-8127-7b09abbdf080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilevelFlow(\n",
    "    m_sq=M_SQ,\n",
    "    lam=LAM,\n",
    "    model_spec=MODEL_SPEC,\n",
    ")\n",
    "dist = torch.distributions.Normal(\n",
    "    loc=torch.zeros((LATTICE_LENGTH, LATTICE_LENGTH)),\n",
    "    scale=torch.ones((LATTICE_LENGTH, LATTICE_LENGTH)),\n",
    ")\n",
    "# dist = FreeScalarDistribution(LATTICE_LENGTH, M_SQ)\n",
    "train_dataloader = Prior(dist, sample_shape=[N_BATCH, 1])\n",
    "val_dataloader = Prior(dist, sample_shape=[N_BATCH_VAL, 1])\n",
    "\n",
    "pbar = utils.JlabProgBar()\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_steps=N_TRAIN,  # total number of training steps\n",
    "    val_check_interval=100,  # how often to run sampling\n",
    "    limit_val_batches=1,  # one batch for each val step\n",
    "    callbacks=[pbar, lr_monitor],\n",
    "    enable_checkpointing=False,  # manually saving checkpoints\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0bf92-6e92-4f61-84ec-a0a56b84ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model, val_dataloader)  # check acceptance before training starts\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a08f4e-f88e-4ec8-8e45-16f806e1690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc79754-13d0-404a-b7ac-487ea15db924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
